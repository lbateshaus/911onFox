---
title: "911 5b-12 TAKE 4"
description: |
  Practice in scraping tweets and making a word cloud.
author:
  - name: Lissie Bates-Haus, Ph.D. 
    url: https://github.com/lbateshaus
    affiliation: U Mass Amherst DACSS MS Student
    affiliation_url: https://www.umass.edu/sbs/data-analytics-and-computational-social-science-program/ms
date: "`r Sys.Date()`"
output: distill::distill_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Load Libraries:

```{r}

#all necessary libraries here

library(rtweet)
library(twitteR)
library(leaflet)
library(quanteda)
library(readr)
library(httr)
library(tidytext)
library(tidyverse)
library(quanteda.textmodels)
library(tm)
library(wordcloud)
library(RColorBrewer)
library(wordcloud2)
library(dplyr)
```

911 on Fox Tweets

Load tweets from where I scraped them earlier:

```{r}
setwd("~/DACCS R/Text as Data/911Fox Project")
#Already scraped tweets so just loading in the csv

tweets5b12 <- read_csv("tweets5b12.csv")
View(tweets5b12)

#I have no idea why this seems to think the file isn't in this directory when it is?
#Fixed it - path issue
```

## Separate the Data into only post-5b 12 tweets.


```{r}

#copy over tweet df to a working df

tweets <- tweets5b12
```

From a little googling, I can see that the Twitter API returns a created-at timestamp in Greenwich Mean Time, which is 4 hours later than my local time, which means I'm looking for tweets after 12 midnight 3/29.2022 (8 pm DST in my timezone).

I used the code provided by [earthdatasciene.org](https://www.earthdatascience.org/courses/earth-analytics/get-data-using-apis/text-mine-colorado-flood-tweets-science-r/) (probably not their intended application but oh well!)

```{r}
#Narrowing down my working df

nrow(tweets)
#format = "%Y-%m-%d %H:%M:%s"
# show start date march 29 12 midnight Greenwich Mean Time
start_date <- as.POSIXct('2022-03-29 00:00:00', tz="UTC")

tweets <- tweets %>% filter(created_at >= start_date)

nrow(tweets)

```

So I've filtered down from 9997 to 4652 tweets.

### Explore Common Words

```{r}

tweetWords <- tweets %>%
  dplyr::select(text) %>%
  unnest_tokens(word, text)

head(tweetWords)

```
Attempt to plot the top 15 words:

```{r}

# plot the top 15 words
tweetWords %>%
  dplyr::count(word, sort = TRUE) %>%
  top_n(15) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
      labs(x = "Count",
      y = "Unique words",
      title = "Count of unique words found in tweets")


```
### Deal with Stop Words

```{r}

data("stop_words")
# how many words do you have including the stop words?
nrow(tweetWords)

tweetsClean <- tweetWords %>%
  anti_join(stop_words) %>%
  filter(!word == "rt")

# how many words after removing the stop words?
nrow(tweetsClean)

```

Replot top 15

```{r}

# plot the top 50 words -- notice any issues?
tweetsClean %>%
  dplyr::count(word, sort = TRUE) %>%
  top_n(50) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
      labs(x = "Count",
      y = "Unique words",
      title = "Count of unique words found in tweets")

```
I still want to get things like https and t.co and 911onfox out of here:

```{r}
#this gets https out I think

nrow(tweetsClean)
# cleanup
tweetsClean <- tweets %>%
  mutate(text = gsub("\\s?(f|ht)(tp)(s?)(://)([^\\.]*)[\\.|/](\\S*)", 
                           "", text)) %>% 
  filter(created_at >= start_date ) %>% 
  dplyr::select(text) %>%
  unnest_tokens(word, text) %>% 
  anti_join(stop_words) %>%
  filter(!word == "rt") # remove all rows that contain "rt" or retweet
nrow(tweetsClean)
```

Replot top 15

```{r}

# plot the top 50 words -- notice any issues?
tweetsClean %>%
  dplyr::count(word, sort = TRUE) %>%
  top_n(50) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
      labs(x = "Count",
      y = "Unique words",
      title = "Count of unique words found in tweets, ")

```
From here I want to remove numbers and words that start with numbers. How do I do that?

```{r}

#I'm going to try gsub - that worked but left an empty cell!

nrow(tweetsClean)

#tweetsCloud <- tweetsClean %>% slice(-("911onfox"))  this doesn't work 

#going to try subset

tweetsCloud <- subset(tweetsClean, word!="911onfox" & word!="episode" & word!="911lonestar" 
                      & word!="hewitt" & word!="i'm" & word!="it's" & word!="1" 
                      &  word!="chim" & word!="gonna" & word!="tonight" 
                      & word!="shes" & word!="im")       #IT LOOKS LIKE THAT WORKED!!



#tweetsCloud <- subset(tweetsCloud, word!="episode")
#tweetsCloud <- subset(tweetsCloud, word!="911lonestar")   #every time I run the word cloud I see words to take out
#tweetsCloud <- subset(tweetsCloud, word!="hewitt") 
#tweetsCloud <- subset(tweetsCloud, word!="im") 
#tweetsCloud <- subset(tweetsCloud, word!="it's") 
#tweetsCloud <- subset(tweetsCloud, word!="1") 
#tweetsCloud <- subset(tweetsCloud, word!="chim") 
#tweetsCloud <- subset(tweetsCloud, word!="gonna")
#tweetsCloud <- subset(tweetsCloud, word!="tonight")
#tweetsCloud <- subset(tweetsCloud, word!="I'm") 

nrow(tweetsCloud)

```

Replot top 60

```{r}

# plot the top 60 words
tweetsCloud %>%
  dplyr::count(word, sort = TRUE) %>%
  top_n(60) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
      labs(x = "Count",
      y = "Unique words",
      title = "Count of unique words found in tweets, ")

```


Finally, word cloud???

```{r}
#Can I put the top 60 words into it's own dataframe?

top50 <- tweetsCloud %>%
  dplyr::count(word, sort = TRUE) %>%
  top_n(54) %>%
  mutate(word = reorder(word, n))

head(top50)
```

```{r}
nrow(top50)

gsub("'", "", top50)

top50a <- subset(top50, word!="im")

nrow(top50a)

```
Okay, at this point, I have no idea why I can't get the words with apostrophes in them OUT in R and my google-fu is failing me, so I'm just going to pull the dataframe down to a csv, edit it in excel and try again.

```{r}
setwd("~/DACCS R/Text as Data/911Fox Project")
write_as_csv(tweetsCloud,"tweetsCloud.csv")

```

Load in the cleaned up csv:

```{r}
 
cleanCloud <- read_csv("tweetsCloud.csv")

```

Plot Top 60

```{r}
# plot the top 60 words
cleanCloud %>%
  dplyr::count(word, sort = TRUE) %>%
  top_n(60) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
      labs(x = "Count",
      y = "Unique words",
      title = "Count of unique words found in tweets, ")
```
Load the top 60 into their own df

```{r}
top60 <- cleanCloud %>%
  dplyr::count(word, sort = TRUE) %>%
  top_n(60) %>%
  mutate(word = reorder(word, n))
```




```{r}

wordcloud2(data=top60, size=2, color = "random-dark")

```

Brief attempt to change the colors:

```{r}
# or a vector of colors. vector must be same length than input data
wordcloud2(top60, size=1.6, color=rep_len( c("mediumblue","darkorchid", "seagreen", "firebrick", "deeppink", "goldenrod"), nrow(top60) ) )
```
For some reason, this 2nd wordcloud isn't visible when I knit this? Huh. I don't know why.

I can't figure out how to export the image? I wonder if I need to run it in a different package? Is this an interactive one?
