---
title: "911 5b-12 TAKE 4"
description: |
  Practice in scraping tweets and making a word cloud.
author:
  - name: Lissie Bates-Haus, Ph.D. 
    url: https://github.com/lbateshaus
    affiliation: U Mass Amherst DACSS MS Student
    affiliation_url: https://www.umass.edu/sbs/data-analytics-and-computational-social-science-program/ms
date: "`r Sys.Date()`"
output: distill::distill_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("~/DACCS R/Text as Data/911Fox Project")
```

Load Libraries:

```{r warning=FALSE}

#all necessary libraries here

library(rtweet)
library(twitteR)
library(leaflet)
library(quanteda)
library(readr)
library(httr)
library(tidytext)
library(tidyverse)
library(quanteda.textmodels)
library(tm)
library(wordcloud)
library(RColorBrewer)
library(wordcloud2)
library(dplyr)

#Copied over the whole doc from 5b-12
```

Please note: this code is almost entirely from my project [911 5b-12 TAKE 4](https://rpubs.com/lbateshaus/889197). New code will be cited as necessary.

## 911 on Fox Tweets

Load tweets from where I scraped them earlier:

```{r warning=FALSE}
setwd("~/DACCS R/Text as Data/911Fox Project")
#Already scraped tweets so just loading in the csv

tweets13 <- read_csv("tweets13.csv")
head(tweets13)

```

### Separate the Data into only post-5b 13 tweets.


```{r}

#copy over tweet df to a working df

workingTweets13 <- tweets13

```

## Narrow down to just post-airing tweets

```{r}
#Narrowing down my working df

nrow(workingTweets13)

#format = "%Y-%m-%d %H:%M:%s"
# show start date march 29 12 midnight Greenwich Mean Time
start_date <- as.POSIXct('2022-03-29 00:00:00', tz="UTC")

workingTweets13 <- workingTweets13 %>% filter(created_at >= start_date)

nrow(workingTweets13)

```

So that's curious, nothing filtered down. Does this mean all 6941 tweets were only published after the show aired? Let's look at our data.

OH - because I forgot to change my start date!


```{r}
#Narrowing down my working df

nrow(workingTweets13)

#format = "%Y-%m-%d %H:%M:%s"
# show start date march 29 12 midnight Greenwich Mean Time
start_date <- as.POSIXct('2022-04-11 00:00:00', tz="UTC")

workingTweets13 <- workingTweets13 %>% filter(created_at >= start_date)

nrow(workingTweets13)

```

So that carved us down a bit - let's check the data.

### Explore Common Words

```{r}

tweetWords <- workingTweets13 %>%
  dplyr::select(text) %>%
  unnest_tokens(word, text)

head(tweetWords)

```
Attempt to plot the top 15 words:

```{r}

# plot the top 15 words
tweetWords %>%
  dplyr::count(word, sort = TRUE) %>%
  top_n(15) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
      labs(x = "Count",
      y = "Unique words",
      title = "Count of unique words found in tweets")


```
### Deal with Stop Words

```{r}

data("stop_words")
# how many words do you have including the stop words?
nrow(tweetWords)

tweetsClean <- tweetWords %>%
  anti_join(stop_words) %>%
  filter(!word == "rt")

# how many words after removing the stop words?
nrow(tweetsClean)

```

Replot top 50

```{r}

# plot the top 50 words -- notice any issues?
tweetsClean %>%
  dplyr::count(word, sort = TRUE) %>%
  top_n(50) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
      labs(x = "Count",
      y = "Unique words",
      title = "Count of unique words found in tweets")

```
I still want to get things like https and t.co and 911onfox out of here:

```{r}
#this gets https out I think

nrow(tweetsClean)
# cleanup
tweetsClean <- workingTweets13 %>%
  mutate(text = gsub("\\s?(f|ht)(tp)(s?)(://)([^\\.]*)[\\.|/](\\S*)", 
                           "", text)) %>% 
  filter(created_at >= start_date ) %>% 
  dplyr::select(text) %>%
  unnest_tokens(word, text) %>% 
  anti_join(stop_words) %>%
  filter(!word == "rt") # remove all rows that contain "rt" or retweet
nrow(tweetsClean)

```

Replot top 50

```{r}

# plot the top 50 words -- notice any issues?
tweetsClean %>%
  dplyr::count(word, sort = TRUE) %>%
  top_n(50) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
      labs(x = "Count",
      y = "Unique words",
      title = "Count of unique words found in tweets, ")

```
From here I want to remove numbers and words that start with numbers. How do I do that?

```{r}

#I'm going to try gsub - that worked but left an empty cell!

nrow(tweetsClean)

#tweetsCloud <- tweetsClean %>% slice(-("911onfox"))  this doesn't work 

#going to try subset

tweetsCloud <- subset(tweetsClean, word!="911onfox" & word!="episode" & word!="911lonestar" 
                      & word!="hewitt" & word!="i'm" & word!="it's" & word!="1" 
                      &  word!="chim" & word!="gonna" & word!="tonight" 
                      & word!="shes" & word!="im")       #IT LOOKS LIKE THAT WORKED!!



#tweetsCloud <- subset(tweetsCloud, word!="episode")
#tweetsCloud <- subset(tweetsCloud, word!="911lonestar")   #every time I run the word cloud I see words to take out
#tweetsCloud <- subset(tweetsCloud, word!="hewitt") 
#tweetsCloud <- subset(tweetsCloud, word!="im") 
#tweetsCloud <- subset(tweetsCloud, word!="it's") 
#tweetsCloud <- subset(tweetsCloud, word!="1") 
#tweetsCloud <- subset(tweetsCloud, word!="chim") 
#tweetsCloud <- subset(tweetsCloud, word!="gonna")
#tweetsCloud <- subset(tweetsCloud, word!="tonight")
#tweetsCloud <- subset(tweetsCloud, word!="I'm") 

nrow(tweetsCloud)

```

Replot top 60

```{r}

# plot the top 60 words
tweetsCloud %>%
  dplyr::count(word, sort = TRUE) %>%
  top_n(60) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
      labs(x = "Count",
      y = "Unique words",
      title = "Count of unique words found in tweets, ")

```
What I figured out from my last project is that I have an encoding problem here, which is why I'm still getting words that aren't meaningful. While it's probably not the correct way to manage this, I'm going to write my top 80 words to a csv, pull it up in excel and take a look at it there. Like I said, probably not ideal, but this will give me 60 words for my wordcloud.

```{r}
#Put the top 80 words into its own dataframe?

top80 <- tweetsCloud %>%
  dplyr::count(word, sort = TRUE) %>%
  top_n(80) %>%
  mutate(word = reorder(word, n))

head(top80)

write_as_csv(top80,"top80.csv")

```

From here, I'll go to excel and clean it up! Note: this is the qualitative part of the analysis

Deleted from top80:

i‚Äôm
don‚Äôt
amp
it‚Äôs
can‚Äôt
he‚Äôs
didn‚Äôt
that‚Äôs

All of these look to me like they would have been removed with stop words?
I think next week, I should practice making ngrams because I bet that would be interesting!

Now to import the csv

```{r}

top73 <- read_csv("top73.csv")
head(top73)

```


Finally, word cloud???

Plot Top 73

```{r}

wordcloud2(data=top73, color = "random-dark")

```
Since I don't love this, I'm going to cull some more words based on my domain-specific knowledge.

scene
season
fucking (removed and total added to "fuck")
watch
watching

Reload the file:

```{r}

top73 <- read_csv("top73.csv")
head(top73)

```

Rerun the wordcloud:

```{r}

wordcloud2(data=top73, color = "random-dark")

```
Brief attempt to change the colors:

```{r}
# or a vector of colors. vector must be same length than input data
wordcloud2(top73, color=rep_len( c("mediumblue","darkorchid", "seagreen", "firebrick", "deeppink", "goldenrod"), nrow(top73) ) )
```
And now to export it as a png!

```{r}
# install webshot
library(webshot)
webshot::install_phantomjs()

# Make the graph
my_graph <- wordcloud2(top73, color=rep_len( c("mediumblue","darkorchid", "seagreen", "firebrick", "deeppink", "goldenrod"), nrow(top73) ) )

# save it in html
library("htmlwidgets")
saveWidget(my_graph,"tmp.html",selfcontained = F)

# and in png or pdf
webshot("tmp.html","fig_1.png", delay =5, vwidth = 480, vheight=480)


```

